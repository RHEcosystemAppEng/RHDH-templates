# Default values for llama-serve.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

imagePullSecrets: []
nameOverride: "vllm"
fullnameOverride: "vllm"

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8000

serviceAccount:
  create: false

args:
  - --enable-auto-tool-choice
  - --chat-template
  - /app/tool_chat_template_llama3.2_json.jinja
  - --tool-call-parser
  - llama3_json
  - --port
  - "8000"
  - '--max-model-len'
  - '30544'

env:
  - name: VLLM_PORT
    value: "8000"
  - name: HUGGING_FACE_HUB_TOKEN
    valueFrom:
      secretKeyRef:
        key: HF_TOKEN
        name: huggingface-secret
# Additional volumes on the output Deployment definition.
volumes:
  - emptyDir: {}
    name: hf-cache
  - configMap:
      defaultMode: 420
      name: template
    name: chat-template
  - emptyDir: {}
    name: config

# Additional volumeMounts on the output Deployment definition.
volumeMounts:
  - mountPath: /.cache
    name: hf-cache
  - mountPath: /app
    name: chat-template
  - mountPath: /.config
    name: config


nodeSelector: {}

affinity: {}
