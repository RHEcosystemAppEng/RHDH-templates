global:
  models:
    llama-3-2-3b-instruct:
      id: meta-llama/Llama-3.2-3B-Instruct
      enabled: false
      inferenceService:
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvName: LLAMA_INSTRUCT3B_MODEL
        modelUrl:
          envName: LLAMA_INSTRUCT3B_VLLM_URL
          envValue: auto
    llama-guard-3-8b:
      id: meta-llama/Llama-Guard-3-8B
      enabled: false
      inferenceService:
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvName: LLAMA_GUARD8B_MODEL
        modelUrl:
          envName: LLAMA_GUARD8B_VLLM_URL
          envValue: auto
        registerShield: true
    llama-3-2-1b-instruct:
      id: meta-llama/Llama-3.2-1B-Instruct
      enabled: false
      inferenceService:
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvName: LLAMA_INSTRUCT1B_MODEL
        modelUrl:
          envName: LLAMA_INSTRUCT1B_VLLM_URL
          envValue: auto
    llama-guard-3-1b:
      id: meta-llama/Llama-Guard-3-1B
      enabled: false
      inferenceService:
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvName: LLAMA_GUARD1B_MODEL
        modelUrl:
          envName: LLAMA_GUARD1B_VLLM_URL
          envValue: auto
        registerShield: true
    llama-3-3-70b-instruct:
      id: meta-llama/Llama-3.3-70B-Instruct
      enabled: false
      storageSize: 150Gi
      inferenceService:
        resources:
          limits:
            nvidia.com/gpu: "4"
        args:
        - --tensor-parallel-size
        - "4"
        - --gpu-memory-utilization
        - "0.95"
        - --quantization
        - fp8
        - --max-num-batched-tokens
        - "4096"
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --swap-space
        - "32"
      llamaStack:
        modelIdEnvName: LLAMA_INSTRUCT70B_MODEL
        modelUrl:
          envName: LLAMA_INSTRUCT70B_VLLM_URL
          envValue: auto

llm-service:
  servingRuntime:
    image: quay.io/ecosystem-appeng/vllm:openai-v0.8.3
    recommendedAccelerators:
    - nvidia.com/gpu
