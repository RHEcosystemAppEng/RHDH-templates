global:
  models:
    llama-3-2-1b-instruct:
      id: meta-llama/Llama-3.2-1B-Instruct
      enabled: false
      inferenceService:
        resources: {}
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /workspace/vllm/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvName: LLAMA_INSTRUCT_MODEL
        modelUrl:
          envName: LLAMA_INSTRUCT_VLLM_URL
          envValue: auto
    llama-guard-3-1b:
      id: meta-llama/Llama-Guard-3-1B
      enabled: false
      inferenceService:
        resources: {}
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvName: LLAMA_GUARD_MODEL
        modelUrl:
          envName: LLAMA_GUARD_VLLM_URL
          envValue: auto
        registerShield: true

llm-service:
  servingRuntime:
    image: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo@sha256:6b0d6af534a3c2a24fbcbd0e5e919f9f215be73fdf799a7bbd324178956cb7df
