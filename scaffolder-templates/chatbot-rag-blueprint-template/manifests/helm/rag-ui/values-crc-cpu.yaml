# Default values for rag-ui.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/yuvalturg/llamastack-dist-ui
  pullPolicy: IfNotPresent
  tag: 0.1.9

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  port: 8501

env:
  - name: LLAMA_STACK_ENDPOINT
    value: 'http://llamastack:8321'
  - name: MCP_SERVERS_CONFIG_FILE
    value: "/app-config/mcp_servers_config.yaml"

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources: {}

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

autoscaling:
  enabled: false

# Additional volumes on the output Deployment definition.
volumes:
  - configMap:
      defaultMode: 420
      name: mcp-servers-config
    name: mcp-servers-config-volume

# Additional volumeMounts on the output Deployment definition.
volumeMounts:
  - mountPath: /app-config
    name: mcp-servers-config-volume
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

global:
  models:
    llama-instruct:
      id: meta-llama/Llama-3.2-1B-Instruct
      inferenceService:
        name: llama-instruct
        storageUri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-1b-instruct
        resources: {}
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /workspace/vllm/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvVar: INFERENCE_MODEL
        modelUrlEnvVar: VLLM_URL
    llama-guard:
      id: meta-llama/Llama-Guard-3-1B
      inferenceService:
        name: llama-guard
        storageUri: oci://quay.io/ecosystem-appeng/modelcars:llama-guard-3-1b
        resources: {}
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvVar: SAFETY_MODEL
        modelUrlEnvVar: SAFETY_VLLM_URL

llm-service:
  servingRuntimeImage: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo@sha256:6b0d6af534a3c2a24fbcbd0e5e919f9f215be73fdf799a7bbd324178956cb7df


## llama-serve and safety-model were replaced by llm-service, set replicaCount=0 ##
llama-serve:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-3.2-1B-Instruct
  tolerations: []
  resources: null
  image:
    repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
    pullPolicy: IfNotPresent
    tag: v0.8.1

safety-model:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-Guard-3-1B
  tolerations: []
  resources: null
  image:
    repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
    pullPolicy: IfNotPresent
    tag: v0.8.1
