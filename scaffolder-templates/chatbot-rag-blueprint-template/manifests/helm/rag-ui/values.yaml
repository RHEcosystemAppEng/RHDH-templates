# Default values for rag-ui.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/ecosystem-appeng/llamastack-dist-ui
  pullPolicy: IfNotPresent
  tag: 0.1.9

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  port: 8501

env:
  - name: LLAMA_STACK_ENDPOINT
    value: 'http://llamastack:8321'
  - name: MCP_SERVERS_CONFIG_FILE
    value: "/app-config/mcp_servers_config.yaml"

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources: {}

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

autoscaling:
  enabled: false

# Additional volumes on the output Deployment definition.
volumes:
  - configMap:
      defaultMode: 420
      name: mcp-servers-config
    name: mcp-servers-config-volume

# Additional volumeMounts on the output Deployment definition.
volumeMounts:
  - mountPath: /app-config
    name: mcp-servers-config-volume
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

global:
  models:
    llama-instruct:
      id: meta-llama/Llama-3.2-3B-Instruct
      inferenceService:
        name: llama-instruct
        storageUri: oci://quay.io/ecosystem-appeng/modelcars:llama-3.2-3b-instruct
        args:
        - --enable-auto-tool-choice
        - --chat-template
        - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
        - --tool-call-parser
        - llama3_json
        - --max-model-len
        - "30544"
      llamaStack:
        modelIdEnvVar: INFERENCE_MODEL
        modelUrlEnvVar: VLLM_URL
    llama-guard:
      id: meta-llama/Llama-Guard-3-8B
      inferenceService:
        name: llama-guard
        storageUri: oci://quay.io/ecosystem-appeng/modelcars:llama-guard-3-8b
        args:
        - --max-model-len
        - "14336"
      llamaStack:
        modelIdEnvVar: SAFETY_MODEL
        modelUrlEnvVar: SAFETY_VLLM_URL

llm-service:
  servingRuntimeImage: quay.io/ecosystem-appeng/vllm:openai-v0.8.3


## llama-serve and safety-model were replaced by llm-service, set replicaCount=0 ##
llama-serve:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-3.2-3B-Instruct
  tolerations:
    - effect: NoSchedule
      key: g6e-gpu
      value: 'true'
  resources:
    limits:
      nvidia.com/gpu: "1"
  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: latest

safety-model:
  replicaCount: 0
  extraArgs:
    - --model
    - meta-llama/Llama-Guard-3-8B
  tolerations:
    - effect: NoSchedule
      key: odh-notebook
      value: 'true'
  resources:
    limits:
      nvidia.com/gpu: "1"
  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: latest
